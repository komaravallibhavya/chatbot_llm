# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13-qfmc8bqAOqN5KGz-4Yn2yYtVZ7wjBR
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import torch
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install --no-deps trl xformers peft accelerate bitsandbytes
#

from unsloth import FastLanguageModel

max_seq_length = 2048
load_in_4bit = True

# Choosing gemma-7b-bnb-4bit for efficient tuning
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/gemma-7b-bnb-4bit",
    max_seq_length=max_seq_length,
    dtype=None,
    load_in_4bit=load_in_4bit
)

import pandas as pd
from datasets import Dataset

# Load the provided fashion dataset
fashion_dataset_path = "/content/fashion_advisor_transformed_dataset.csv"
fashion_data = pd.read_csv(fashion_dataset_path)

# Convert to HuggingFace dataset format
fashion_dataset = Dataset.from_pandas(fashion_data)

import torch
from unsloth import FastLanguageModel
from transformers import TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset, Dataset
import pandas as pd

# Load the fashion dataset
file_path = "/content/fashion_advisor_transformed_dataset.csv"
data = pd.read_csv(file_path)

# Prepare the dataset for training
def prepare_fashion_prompts(data):
    """Prepares dataset into prompt-response format."""
    prompts = []
    for _, row in data.iterrows():
        instruction = row.get("instruction", "Provide fashion advice.")
        input_context = row.get("input", "")
        response = row.get("response", "")
        prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_context}\n\n### Response:\n{response}\n"
        prompts.append({"text": prompt})
    return prompts

dataset = Dataset.from_pandas(pd.DataFrame(prepare_fashion_prompts(data)))

# Load the model
max_seq_length = 2048
load_in_4bit = True
model_name = "unsloth/llama-3-8b-bnb-4bit"

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=None,
    load_in_4bit=load_in_4bit
)

model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    max_seq_length=max_seq_length,
    use_rslora=False,
    loftq_config=None
)

# Training arguments
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        max_steps=60,
        num_train_epochs=4,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs"
    )
)

# Train the model
trainer_stats = trainer.train()

# Save the trained model
model.save_pretrained("fashion_advisor_model")

# Create a chatbot interface
def fashion_advisor_chatbot():
    FastLanguageModel.for_inference(model)

    print("Fashion Advisor Chatbot is ready! Type 'exit' to end the chat.")
    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            print("Goodbye!")
            break

        prompt = f"### Instruction:\nProvide fashion advice.\n\n### Input:\n{user_input}\n\n### Response:\n"
        inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

        outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)
        response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

        print(f"Fashion Advisor: {response.strip()}")

# Run the chatbot
if __name__ == "__main__":
    fashion_advisor_chatbot()